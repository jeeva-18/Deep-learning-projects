{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeeva-18/Deep-learning-projects/blob/main/101_foodvisionmodel_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "kkXUQf4UwNyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_datasets = tfds.list_builders()\n",
        "print('food101' in list_datasets) # checking for our dataset presence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ntTirqbxO3j",
        "outputId": "4ed4561f-4af9-4c9f-8b92-e5bfe5e82f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# it will download the data into our sytems if you are using your local computer it will download lots of data  \n",
        "(train_data,test_data),df_info = tfds.load('food101',\n",
        "                                           split=['train','validation'], # in our case we have training and validation but some dataset have testing too\n",
        "                                           shuffle_files=True, # we are shuffle our files to some randomness in our data \n",
        "                                           as_supervised=True, # beacuse our dataset is supervised \n",
        "                                           with_info=True) # getting metadata about our dataset"
      ],
      "metadata": {
        "id": "aRiUHHLGxePh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#features of the dataset\n",
        "df_info.features"
      ],
      "metadata": {
        "id": "gWHuwo3QzscX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the class names \n",
        "\n",
        "class_names = df_info.features['label'].names\n",
        "class_names[:10]"
      ],
      "metadata": {
        "id": "r9aMWkPh4s3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see some info about images and labels\n",
        "for img ,labels in train_data.take(1): # .take method will take one sample from the data and every time we rn these we get random values because we set shuffle = True \n",
        "  print(f'''\n",
        "  image shape: {img.shape}\n",
        "  image dtype: {img.dtype}\n",
        "  label : {labels}\n",
        "  label name: {class_names[labels.numpy()]}\n",
        "  '''\n",
        "  )"
      ],
      "metadata": {
        "id": "1pdtTrJw9mji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "for i,(img ,labels) in enumerate(train_data.take(9)):\n",
        "  plt.subplot(3,3,i+1)\n",
        "  plt.imshow(img)\n",
        "  plt.title(class_names[labels.numpy()])\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "XIYBx2Sb_UbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets create a function that will  preprocess our data\n",
        "\n",
        "def preprocess_image_data(images,labels,img_size=(224,224)):\n",
        "  '''\n",
        "  This function will reduce image size and change dtype int float32\n",
        "\n",
        "  '''\n",
        "  image  = tf.image.resize(images,img_size)\n",
        "\n",
        "  return tf.cast(image,dtype=tf.float32),labels"
      ],
      "metadata": {
        "id": "7yhBE1doDr1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check our function \n",
        "\n",
        "prep_img ,label = preprocess_image_data(img,labels)\n",
        "\n",
        "print(f'image_shape : {prep_img.shape} image_dtype : {prep_img.dtype} ')"
      ],
      "metadata": {
        "id": "uTpm0QG5MmiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we are going to use map method to map our preprocess_image_data function for preprocessing\n",
        "\n",
        "train_data = train_data.map(map_func=preprocess_image_data,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# making into batches for training data\n",
        "\n",
        "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# preprocess and making batches for testing data\n",
        "\n",
        "test_data  = test_data.map(map_func=preprocess_image_data,num_parallel_calls=tf.data.AUTOTUNE).batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "hjqOwfXSNNWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data,test_data"
      ],
      "metadata": {
        "id": "zsDMQThIRPqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import datetime\n",
        "# creating the tensoboard callback\n",
        "def create_tensorboard_callback(dir_name,experiment):\n",
        "  date_time = datetime.datetime.now().strftime('%Y/%m/%d:%H-%M-%S')\n",
        "  path = os.path.join(dir_name,experiment,date_time)\n",
        "  return tf.keras.callbacks.TensorBoard(log_dir=path)"
      ],
      "metadata": {
        "id": "wGTG8Klc9YZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model check point callback\n",
        "\n",
        "checkpoint_path = 'model_checkpoints/cp.cpkt'\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                         monitor='val_accuracy',\n",
        "                                                         save_best_only=True,\n",
        "                                                         save_weights_only=True,\n",
        "                                                         verbose=0)"
      ],
      "metadata": {
        "id": "K-tQ3LmxGAec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start by importing it \n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "mixed_precision.set_global_policy('mixed_float16') # this will do the job"
      ],
      "metadata": {
        "id": "779CbmedHbMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L  # offcourse mixed precision will only works on the gpu with computing capability of above 7.0, likely our's tesla t4 got 7.5 :)  "
      ],
      "metadata": {
        "id": "1mgCm_cbJQUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mixed_precision.global_policy() # you can use this we utilize large amount of data"
      ],
      "metadata": {
        "id": "AuAqiIvsJgcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing \n",
        "\n",
        "# we are going t use the transfer learning - feature extraction\n",
        "\n",
        "input_shape = (224,224,3)\n",
        "\n",
        "# our base model will be EfficientNetB0\n",
        "base_model = tf.keras.applications.EfficientNetV2B0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "input = layers.Input(shape=input_shape,name='input_layer')\n",
        "\n",
        "# data_augmentation = tf.keras.Sequential([ # this is for  data augmentation\n",
        "#     preprocessing.RandomRotation(20),\n",
        "#     preprocessing.RandomZoom(0.2),\n",
        "#     preprocessing.RandomFlip(mode='horizontal'), \n",
        "#     preprocessing.RandomWidth(0.2),\n",
        "#     preprocessing.RandomHeight(0.2),\n",
        "#     preprocessing.Rescaling(1/255.)      \n",
        "# ])(input)\n",
        "\n",
        "x = base_model(input)\n",
        "\n",
        "x = layers.GlobalAveragePooling2D(name='globalAvgPooling_layer')(x)\n",
        "\n",
        "x = layers.Dense(101,name='output_layer')(x)\n",
        "\n",
        "output = layers.Activation(activation='softmax',dtype=tf.float32,name='prediction')(x)\n",
        "\n",
        "model = tf.keras.Model(input,output,name='food_vision_model')\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "juAZD4wPMVUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "jDLklEGTTQQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mixed precision works or not \n",
        "\n",
        "for layer in model.layers:\n",
        "  print(layer.name,layer.trainable,layer.dtype,layer.dtype_policy)"
      ],
      "metadata": {
        "id": "KrFhZzyPTUbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check it in our base model\n",
        "\n",
        "for layer in model.layers[1].layers[:20]:\n",
        "  print(layer.name,layer.trainable,layer.dtype,layer.dtype_policy)"
      ],
      "metadata": {
        "id": "F0ZNGLy6kQwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extraction_history = model.fit(train_data,\n",
        "                                       epochs=5,\n",
        "                                       steps_per_epoch=len(train_data),\n",
        "                                       validation_data=test_data,\n",
        "                                       validation_steps=int(.15*len(test_data)),\n",
        "                                       callbacks=[create_tensorboard_callback('models','FX_efficientnet0'),checkpoint_callback])"
      ],
      "metadata": {
        "id": "p0BLD0DJkdj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fx_results = model.evaluate(test_data)\n",
        "fx_results"
      ],
      "metadata": {
        "id": "XRCg2YarljSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we are going to create a plot of history of the model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "  \"\"\"\n",
        "  this will give you the plot of the history instance\n",
        "  \"\"\"\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  # plot accuracy \n",
        "  plt.figure()\n",
        "  plt.plot(acc,label='Training')\n",
        "  plt.plot(val_acc,label='validation')\n",
        "  plt.title('Accuracy scores')\n",
        "  plt.legend()\n",
        "  #plot loss\n",
        "  plt.figure()\n",
        "  plt.plot(loss,label='Training')\n",
        "  plt.plot(val_loss,label='validation')\n",
        "  plt.title('Losses')\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "kM1zdcfnoCxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(feature_extraction_history)"
      ],
      "metadata": {
        "id": "5eJsmQ2up-wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets make it up\n",
        "\n",
        "base_model.trainable = True\n",
        "\n",
        "\n",
        "for layer in base_model.layers[:-20]:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "YLikYOfnqHBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4), # we are reducing the learning rate so pretrained weights does not change too much \n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "xOn_umvAsbYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "  print(layer.name,layer.trainable)"
      ],
      "metadata": {
        "id": "p_j7UJwCsy2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for num,layer in enumerate(model.layers[1].layers):\n",
        "  print(num,layer.name,layer.trainable)"
      ],
      "metadata": {
        "id": "udjs1yPIs-4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
        "                                                  patience=3) # if val loss decreases for 3 epochs in a row, stop training\n",
        "\n",
        "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
        "checkpoint_path = \"fine_tune_checkpoints/\"\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      save_best_only=True,\n",
        "                                                      monitor=\"val_loss\")"
      ],
      "metadata": {
        "id": "p5_o0oQYaDXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the callbacks to reduce learning rate of the model\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
        "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, # print out when learning rate goes down \n",
        "                                                 min_lr=1e-7)"
      ],
      "metadata": {
        "id": "d3L9oO0taL9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = model.fit(train_data,\n",
        "                       epochs=100,\n",
        "                       steps_per_epoch=len(train_data),\n",
        "                       validation_data=test_data,\n",
        "                       validation_steps=int(.15 * len(test_data)),\n",
        "                       callbacks=[create_tensorboard_callback('models','best_fine_effb0'),\n",
        "                                  early_stopping,reduce_lr,model_checkpoint])"
      ],
      "metadata": {
        "id": "L5ax1dUns-uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_results = model.evaluate(test_data)\n",
        "best_results"
      ],
      "metadata": {
        "id": "JdFZK08A_FQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(best_model)"
      ],
      "metadata": {
        "id": "KaHCcd3Yq9ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TlDZ16pjrAy3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}